"use strict";(self.webpackChunkkb=self.webpackChunkkb||[]).push([[8661],{3905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>f});var i=r(7294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,i)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,i,n=function(e,t){if(null==e)return{};var r,i,n={},a=Object.keys(e);for(i=0;i<a.length;i++)r=a[i],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)r=a[i],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var l=i.createContext({}),m=function(e){var t=i.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},c=function(e){var t=m(e.components);return i.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},p=i.forwardRef((function(e,t){var r=e.components,n=e.mdxType,a=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=m(r),f=n,h=p["".concat(l,".").concat(f)]||p[f]||u[f]||a;return r?i.createElement(h,o(o({ref:t},c),{},{components:r})):i.createElement(h,o({ref:t},c))}));function f(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var a=r.length,o=new Array(a);o[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:n,o[1]=s;for(var m=2;m<a;m++)o[m]=r[m];return i.createElement.apply(null,o)}return i.createElement.apply(null,r)}p.displayName="MDXCreateElement"},5294:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>m});var i=r(7462),n=(r(7294),r(3905));const a={},o="Transformers in Vision",s={unversionedId:"ml/transformers/vision-transformers",id:"ml/transformers/vision-transformers",title:"Transformers in Vision",description:"[2209.07399] A Light Recipe to Train Robust Vision Transformers",source:"@site/docs/01-ml/transformers/vision-transformers.mdx",sourceDirName:"01-ml/transformers",slug:"/ml/transformers/vision-transformers",permalink:"/docs/ml/transformers/vision-transformers",draft:!1,editUrl:"https://github.com/michalwols/kb/edit/master/docs/01-ml/transformers/vision-transformers.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Intro",permalink:"/docs/ml/transformers/"},next:{title:"Training Tricks",permalink:"/docs/ml/tricks"}},l={},m=[{value:"Architectures",id:"architectures",level:2},{value:"ViT",id:"vit",level:3},{value:"Swin",id:"swin",level:3},{value:"DEIT",id:"deit",level:3},{value:"CiT",id:"cit",level:3},{value:"Training",id:"training",level:2},{value:"Small Data",id:"small-data",level:3},{value:"Detection",id:"detection",level:2},{value:"VitDet",id:"vitdet",level:3},{value:"Code",id:"code",level:2},{value:"Videos",id:"videos",level:2},{value:"Papers",id:"papers",level:2}],c={toc:m};function u(e){let{components:t,...r}=e;return(0,n.kt)("wrapper",(0,i.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"transformers-in-vision"},"Transformers in Vision"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2209.07399"},"[2209.07399] A Light Recipe to Train Robust Vision Transformers")),(0,n.kt)("h2",{id:"architectures"},"Architectures"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/cmhungsteve/Awesome-Transformer-Attention"},"https://github.com/cmhungsteve/Awesome-Transformer-Attention")),(0,n.kt)("h3",{id:"vit"},"ViT"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://yurkovak.medium.com/vision-transformer-vit-under-the-magnifying-glass-part-1-70be8d6661a7"},"Vision Transformer (ViT) under the magnifying glass, Part 1 | by Kate Yurkova | Medium")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer"},"Transformers-Tutorials/VisionTransformer at master \xb7 NielsRogge/Transformers-Tutorials \xb7 GitHub"))),(0,n.kt)("h3",{id:"swin"},"Swin"),(0,n.kt)("h3",{id:"deit"},"DEIT"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/deit"},"GitHub - facebookresearch/deit: Official DeiT repository")),(0,n.kt)("h3",{id:"cit"},"CiT"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/CiT/tree/main"},'GitHub - facebookresearch/CiT: Code for the paper titled "CiT Curation in Training for Effective Vision-Language Data".')),(0,n.kt)("h2",{id:"training"},"Training"),(0,n.kt)("h3",{id:"small-data"},"Small Data"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/hananshafi/vits-for-small-scale-datasets"},"https://github.com/hananshafi/vits-for-small-scale-datasets")),(0,n.kt)("h2",{id:"detection"},"Detection"),(0,n.kt)("h3",{id:"vitdet"},"VitDet"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet#lvis"},"detectron2/projects/ViTDet at main \xb7 facebookresearch/detectron2 \xb7 GitHub")),(0,n.kt)("h1",{id:"links"},"Links"),(0,n.kt)("h2",{id:"code"},"Code"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/lucidrains/vit-pytorch"},"GitHub - lucidrains/vit-pytorch: Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/ShoufaChen/AdaptFormer"},'GitHub - ShoufaChen/AdaptFormer: [NeurIPS 2022] Implementation of "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition"'))),(0,n.kt)("h2",{id:"videos"},"Videos"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=J-utjBdLCTo"},"Transformers in Vision: From Zero to Hero (2021)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.youtube.com/playlist?list=PLki3HkfgNEsKa0vP-mZCfWccEFyrT93y_"},"Workshop on Attention and Transformers in Vision | CVPR 2022")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=QdGWCUOO6xw"},"Beyond Convolutional Neural Networks | CVPR 2022 Tutorial"))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/SwinTransformer/AiT"},"GitHub - SwinTransformer/AiT"))),(0,n.kt)("h2",{id:"papers"},"Papers"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2009.06732"},"Efficient Transformers: A Survey")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/Sense-X/SiT"},"GitHub - Sense-X/SiT: Official implementation of \u201cSelf-slimmed Vision Transformer\u201d (ECCV2022)"))))}u.isMDXComponent=!0}}]);