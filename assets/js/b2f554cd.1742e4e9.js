"use strict";(self.webpackChunkkb=self.webpackChunkkb||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2022/12/17/ TIL","metadata":{"permalink":"/blog/2022/12/17/ TIL","editUrl":"https://github.com/michalwols/kb/edit/master/blog/2022-12-17 TIL.md","source":"@site/blog/2022-12-17 TIL.md","title":"TIL;DR: CLIP Scaling","description":"openclip #tildr","date":"2022-12-17T00:00:00.000Z","formattedDate":"December 17, 2022","tags":[],"readingTime":0.45,"hasTruncateMarker":false,"authors":[],"frontMatter":{}},"content":"#clip #openclip #tildr\\n\\n### [Accelerating Self-Supervised Learning via Efficient Training Strategies](https://arxiv.org/abs/2212.05611)\\n\\n\\n### [Reproducible scaling laws for contrastive language-image learning](https://arxiv.org/abs/2212.07143)\\n\\n1. All previous scaling law research use:\\n\\t1. private data\\n\\t2. language modeling or vision unimodal tasks\\n2. This paper uses:\\n\\t1. CLIP contrastive image-language pretraining\\n\\t2. LAION public dataset\\n\\nbatch size 86-88K, on 1520 A100 GPUs, using pytorch DDP\\n\\nAdamW b1=0.9 b2 = 0.98 weight decay = 0.2\\n\\nInfoNCE loss\\n\\nbfloat16\\n\\nTLDR: it scales\\n\\n[GitHub - LAION-AI/scaling-laws-openclip: Reproducible scaling laws for contrastive language-image learning](https://github.com/LAION-AI/scaling-laws-openclip)\\n[GitHub - LAION-AI/CLIP_benchmark: CLIP-like model evaluation](https://github.com/LAION-AI/CLIP_benchmark)"}]}')}}]);