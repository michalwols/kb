"use strict";(self.webpackChunkkb=self.webpackChunkkb||[]).push([[7509],{3905:(e,t,r)=>{r.d(t,{Zo:()=>u,kt:()=>m});var n=r(7294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},i=Object.keys(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var p=n.createContext({}),f=function(e){var t=n.useContext(p),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},u=function(e){var t=f(e.components);return n.createElement(p.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},s=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,i=e.originalType,p=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),s=f(r),m=a,g=s["".concat(p,".").concat(m)]||s[m]||c[m]||i;return r?n.createElement(g,o(o({ref:t},u),{},{components:r})):n.createElement(g,o({ref:t},u))}));function m(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=r.length,o=new Array(i);o[0]=s;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var f=2;f<i;f++)o[f]=r[f];return n.createElement.apply(null,o)}return n.createElement.apply(null,r)}s.displayName="MDXCreateElement"},1725:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>f});var n=r(7462),a=(r(7294),r(3905));const i={},o="Fine Tuning / Transfer Learning",l={unversionedId:"ml/fine-tuning",id:"ml/fine-tuning",title:"Fine Tuning / Transfer Learning",description:"Parameter efficient transfer learning",source:"@site/docs/01-ml/fine-tuning.md",sourceDirName:"01-ml",slug:"/ml/fine-tuning",permalink:"/docs/ml/fine-tuning",draft:!1,editUrl:"https://github.com/michalwols/kb/edit/master/docs/01-ml/fine-tuning.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Few Shot Learning",permalink:"/docs/ml/few-shot"},next:{title:"graphs",permalink:"/docs/ml/graphs"}},p={},f=[{value:"Parameter efficient transfer learning",id:"parameter-efficient-transfer-learning",level:2},{value:"Papers",id:"papers",level:4}],u={toc:f};function c(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,n.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"fine-tuning--transfer-learning"},"Fine Tuning / Transfer Learning"),(0,a.kt)("h2",{id:"parameter-efficient-transfer-learning"},"Parameter efficient transfer learning"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2303.15647"},"[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/calpt/awesome-adapter-resources"},"GitHub - calpt/awesome-adapter-resources: Collection of Tools and Papers related to Adapters (aka Parameter-Efficient Transfer Learning/ Fine-Tuning)")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/microsoft/LoRA"},'GitHub - microsoft/LoRA: Code for loralib, an implementation of "LoRA: Low-Rank Adaptation of Large Language Models"')),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/huggingface/peft"},"GitHub - huggingface/peft: \ud83e\udd17 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning."))),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("a",{parentName:"p",href:"https://huggingface.co/blog/peft"},"Parameter-Efficient Fine-Tuning using \ud83e\udd17 PEFT"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2310.08659"},"[2310.08659] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2305.14314"},"[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2307.05695"},"[2307.05695] Stack More Layers Differently: High-Rank Training Through Low-Rank Updates")))),(0,a.kt)("h4",{id:"papers"},"Papers"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2304.04947"},"[2304.04947] Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference"))))}c.isMDXComponent=!0}}]);