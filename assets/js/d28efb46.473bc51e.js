"use strict";(self.webpackChunkkb=self.webpackChunkkb||[]).push([[7586],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>d});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var p=n.createContext({}),m=function(e){var t=n.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},u=function(e){var t=m(e.components);return n.createElement(p.Provider,{value:t},e.children)},s={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),c=m(a),d=r,g=c["".concat(p,".").concat(d)]||c[d]||s[d]||i;return a?n.createElement(g,l(l({ref:t},u),{},{components:a})):n.createElement(g,l({ref:t},u))}));function d(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=c;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o.mdxType="string"==typeof e?e:r,l[1]=o;for(var m=2;m<i;m++)l[m]=a[m];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},2882:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>s,frontMatter:()=>i,metadata:()=>o,toc:()=>m});var n=a(7462),r=(a(7294),a(3905));const i={},l=void 0,o={unversionedId:"ml/papers/2023-12-04 - MobileCLIP - Fast Image-Text Models through Multi-Modal Reinforced Training",id:"ml/papers/2023-12-04 - MobileCLIP - Fast Image-Text Models through Multi-Modal Reinforced Training",title:"2023-12-04 - MobileCLIP - Fast Image-Text Models through Multi-Modal Reinforced Training",description:"vision-language #mobile",source:"@site/docs/01-ml/papers/2023-12-04 - MobileCLIP - Fast Image-Text Models through Multi-Modal Reinforced Training.md",sourceDirName:"01-ml/papers",slug:"/ml/papers/2023-12-04 - MobileCLIP - Fast Image-Text Models through Multi-Modal Reinforced Training",permalink:"/docs/ml/papers/2023-12-04 - MobileCLIP - Fast Image-Text Models through Multi-Modal Reinforced Training",draft:!1,editUrl:"https://github.com/michalwols/kb/edit/master/docs/01-ml/papers/2023-12-04 - MobileCLIP - Fast Image-Text Models through Multi-Modal Reinforced Training.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"2023-04-14 - Combined Scaling for Zero-shot Transfer Learning",permalink:"/docs/ml/papers/2023-04-14 - Combined Scaling for Zero-shot Transfer Learning"},next:{title:"ML Paper Reading List",permalink:"/docs/ml/papers/reading-list"}},p={},m=[],u={toc:m};function s(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"#clip #vision-language #mobile"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Dataset Augmentation",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"CoCa for Captions (multiple per image), in addition to source captions"),(0,r.kt)("li",{parentName:"ul"},"Text and Image Embeddings from larger CLIP Models",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"embed multiple augmented versions of the images and synthetic captions"),(0,r.kt)("li",{parentName:"ul"},"use multiple CLIP models in ensemble"),(0,r.kt)("li",{parentName:"ul"},"store augmentation params and use them at train time to reproduce augmented version of image"))))),(0,r.kt)("li",{parentName:"ul"},"Loss",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"CLIP loss + distillation term"),(0,r.kt)("li",{parentName:"ul"},"compute on real and synth data and sum up for final loss"))),(0,r.kt)("li",{parentName:"ul"},"Models",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Text-RepMixer",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"![","[Pasted image 20231205123930.png]","]"))),(0,r.kt)("li",{parentName:"ul"},"Vision - FastViT variant called MCi",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},'reduce MLP expansion ratio from 4 to 3, because of "significant amount of redundancy in linear layers", make the model deeper instead'),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"MCi2 matches FastViT on ImageNet (84.5%) while being 15% faster and 14.3% smaller")))))),(0,r.kt)("li",{parentName:"ul"},"Training",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"12M",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"8 A100s"),(0,r.kt)("li",{parentName:"ul"},"8,192 Batch Size"))),(0,r.kt)("li",{parentName:"ul"},"1B",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"256 A100s"),(0,r.kt)("li",{parentName:"ul"},"65,536 Batch Size"))),(0,r.kt)("li",{parentName:"ul"},"Dataset Reinforcement",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"5 synthetic captions per image using the ",(0,r.kt)("inlineCode",{parentName:"li"},"coca_ViT-L-14")," model in OpenCLIP"),(0,r.kt)("li",{parentName:"ul"},"concatenate two CLIP image embeddings (datacomp and openai ViT-L-14)"),(0,r.kt)("li",{parentName:"ul"},"store in Bfloat16"),(0,r.kt)("li",{parentName:"ul"},"use gzipped pickle"))),(0,r.kt)("li",{parentName:"ul"},"Strong Augmentation "))),(0,r.kt)("li",{parentName:"ul"},"Inference",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"iPhone 12 with CoreML")))),(0,r.kt)("p",null,"![","[Pasted image 20231205130408.png]","]\n![","[Pasted image 20231205130246.png]","]"),(0,r.kt)("hr",null),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Ideas",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Captioning model that takes image and source caption when generating new captions ",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"potentially also use nearest neighbors")))))))}s.isMDXComponent=!0}}]);