"use strict";(self.webpackChunkkb=self.webpackChunkkb||[]).push([[7969],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>h});var i=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,r=function(e,t){if(null==e)return{};var n,i,r={},a=Object.keys(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=i.createContext({}),c=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return i.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(n),h=r,m=u["".concat(l,".").concat(h)]||u[h]||d[h]||a;return n?i.createElement(m,o(o({ref:t},p),{},{components:n})):i.createElement(m,o({ref:t},p))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,o=new Array(a);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var c=2;c<a;c++)o[c]=n[c];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},4499:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var i=n(7462),r=(n(7294),n(3905));const a={},o="Model Distillation and Transfer Learning",s={unversionedId:"ml/cv/distill",id:"ml/cv/distill",title:"Model Distillation and Transfer Learning",description:"Knowledge Distillation",source:"@site/docs/ml/cv/distill.mdx",sourceDirName:"ml/cv",slug:"/ml/cv/distill",permalink:"/docs/ml/cv/distill",draft:!1,editUrl:"https://github.com/michalwols/kb/edit/master/docs/ml/cv/distill.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Diffusion Models",permalink:"/docs/ml/cv/diffusion"},next:{title:"Few Shot Learning",permalink:"/docs/ml/cv/few-shot"}},l={},c=[{value:"Knowledge Distillation",id:"knowledge-distillation",level:2},{value:"Function Matching",id:"function-matching",level:3},{value:"Convpass for ViT Tuning",id:"convpass-for-vit-tuning",level:2},{value:"Links",id:"links",level:2}],p={toc:c};function d(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,i.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"model-distillation-and-transfer-learning"},"Model Distillation and Transfer Learning"),(0,r.kt)("h2",{id:"knowledge-distillation"},"Knowledge Distillation"),(0,r.kt)("p",null,"Notes from ",(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2106.05237"},"Knowledge distillation: A good teacher is patient and consistent"),":"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Fixed teacher"),"  We explore several options where the teacher\u2019s predictions are constant for a given image (precomputed target). The simplest (and worst) method is fix/rs, where the image is just resized to 2242px for both student and teacher. fix/cc follows a more common approach of using a fixed central crop for the teacher and a mild random crop for the student. fix/ic ens is a heavy data augmentation approach where the teacher\u2019s prediction is the average of 1k incep- tion crops, which we verified to improve the teacher\u2019s performance. The student also uses random inception crops. The two latter settings are similar to the input noise strategy from the \u201cnoisy student\u201d paper ")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Consistent teaching")," In this approach, we randomly crop the image only once, either with mild random cropping (same/rc) or heavy inception crop (same/ic), and use this same crop for the input to both the student and the teacher.")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Function matching")," This approach extends consistent teaching, by expanding an input manifold of images through ",(0,r.kt)("strong",{parentName:"p"},"mixup")," (mix), and, again, providing consistent inputstothestudentandtheteacher. Forbrevity,we sometimes refer to this approach as \u201cFunMatch\u201d\nping (same/rc) or heavy inception crop (same/ic), and use this same crop for the input to both the student and the teacher.")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"One needs patience along with consistency when doing distillation. Eventually, the teacher will be matched; this is true across various datasets of different scale.")),(0,r.kt)("h3",{id:"function-matching"},"Function Matching"),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(5106).Z,width:"1052",height:"1274"})),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(8747).Z,width:"1012",height:"1422"})),(0,r.kt)("h2",{id:"convpass-for-vit-tuning"},"Convpass for ViT Tuning"),(0,r.kt)("p",null,"Tuning large transformer models is expensive. Convpass adds CNN residual stems that can be tuned for new tasks."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/JieShibo/PETL-ViT"},"GitHub - JieShibo/PETL-ViT: Source code of Convpass \u201cConvolutional Bypasses Are Better Vision Transformer Adapters\u201d")),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(805).Z,width:"2482",height:"1484"})),(0,r.kt)("p",null,(0,r.kt)("img",{src:n(4227).Z,width:"3524",height:"1250"})),(0,r.kt)("h2",{id:"links"},"Links"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/Alibaba-MIIL/Solving_ImageNet"},"GitHub - Alibaba-MIIL/Solving_ImageNet: Official PyTorch implementation of  the paper: \u201cSolving ImageNet: a Unified Scheme for Training any Backbone to Top Results\u201d (2022)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.05237"},"Knowledge distillation: A good teacher is patient and consistent")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/google-research/big_transfer"},"GitHub - google-research/big_transfer: Official repository for the \u201cBig Transfer (BiT): General Visual Representation Learning\u201d paper."))))}d.isMDXComponent=!0},4227:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/distill-convpass-few-shot-9ad910b2fd0bbd4216c1ca895447a491.png"},805:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/distill-convpass-910bc48ed6e60d6dc382aa81816e57a5.png"},5106:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/distill-function-matching-1afd8be6fee20be0df486f3966011e00.png"},8747:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/distill-matching-vs-labels-858aa72ee1befe1caed845e7815f0c8b.png"}}]);