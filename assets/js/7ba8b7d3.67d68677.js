"use strict";(self.webpackChunkkb=self.webpackChunkkb||[]).push([[6007],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>g});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function p(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=r.createContext({}),s=function(e){var t=r.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=s(e.components);return r.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},c=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,u=p(e,["components","mdxType","originalType","parentName"]),c=s(a),g=n,h=c["".concat(l,".").concat(g)]||c[g]||m[g]||i;return a?r.createElement(h,o(o({ref:t},u),{},{components:a})):r.createElement(h,o({ref:t},u))}));function g(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=c;var p={};for(var l in t)hasOwnProperty.call(t,l)&&(p[l]=t[l]);p.originalType=e,p.mdxType="string"==typeof e?e:n,o[1]=p;for(var s=2;s<i;s++)o[s]=a[s];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}c.displayName="MDXCreateElement"},5327:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>p,toc:()=>s});var r=a(7462),n=(a(7294),a(3905));const i={},o="Vision Language Pre Training",p={unversionedId:"ml/vision-language",id:"ml/vision-language",title:"Vision Language Pre Training",description:'GitHub - microsoft/BridgeTower Building Bridges Between Encoders in Vision-Language Representation Learning"',source:"@site/docs/01-ml/vision-language.md",sourceDirName:"01-ml",slug:"/ml/vision-language",permalink:"/docs/ml/vision-language",draft:!1,editUrl:"https://github.com/michalwols/kb/edit/master/docs/01-ml/vision-language.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Video",permalink:"/docs/ml/video"},next:{title:"Visual Search / CBIR / Instance Recognition / Instance Retrieval / Metric Learning",permalink:"/docs/ml/visual-search"}},l={},s=[{value:"Contrastive",id:"contrastive",level:2},{value:"CLIP",id:"clip",level:3},{value:"Vision to Language",id:"vision-to-language",level:2}],u={toc:s};function m(e){let{components:t,...a}=e;return(0,n.kt)("wrapper",(0,r.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"vision-language-pre-training"},"Vision Language Pre Training"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/microsoft/BridgeTower"},'GitHub - microsoft/BridgeTower: Open source code for AAAI 2023 Paper "BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning"')),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/microsoft/react"},"GitHub - microsoft/react")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/salesforce/lavis"},"https://github.com/salesforce/lavis")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/uta-smile/TCL"},"https://github.com/uta-smile/TCL")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/YehLi/xmodaler"},"https://github.com/YehLi/xmodaler")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/sangminwoo/awesome-vision-and-language"},"https://github.com/sangminwoo/awesome-vision-and-language")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2208.02131"},"Masked Vision and Language Modeling for Multi-modal Representation Learning (2022-08-03)")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/CiT/tree/main"},'GitHub - facebookresearch/CiT: Code for the paper titled "CiT Curation in Training for Effective Vision-Language Data".')),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/guilk/VLC"},'GitHub - guilk/VLC: Research code for "Training Vision-Language Transformers from Captions Alone"')),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/OliverRensu/TinyMIM"},"GitHub - OliverRensu/TinyMIM")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/RERV/UniAdapter"},"GitHub - RERV/UniAdapter"),"![","[Screen Shot 2023-04-19 at 1.13.36 PM.png]"),(0,n.kt)("h2",{id:"contrastive"},"Contrastive"),(0,n.kt)("h3",{id:"clip"},"CLIP"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/mlfoundations/open_clip"},"https://github.com/mlfoundations/open_clip"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/baaivision/EVA"},"GitHub - baaivision/EVA: EVA Series: Vision Foundation Model Fanatics from BAAI"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/OpenAI/CLIP"},"https://github.com/OpenAI/CLIP"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/LAION-AI/CLIP_benchmark"},"https://github.com/LAION-AI/CLIP_benchmark"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/rom1504/clip-retrieval"},"https://github.com/rom1504/clip-retrieval"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/rom1504/laion-prepro"},"https://github.com/rom1504/laion-prepro"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/yzhuoning/Awesome-CLIP"},"GitHub - yzhuoning/Awesome-CLIP: Awesome list for research on CLIP (Contrastive Language-Image Pre-Training)."))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"http://arxiv.org/abs/2207.00208"},"e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce")))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/SLIP"},"GitHub - facebookresearch/SLIP: Code release for SLIP Self-supervision meets Language-Image Pre-training"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/flip"},'GitHub - facebookresearch/flip: Official Open Source code for "Scaling Language-Image Pre-training via Masking"'))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/diht"},"GitHub - facebookresearch/diht: Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/OliverRensu/DeepMIM"},"GitHub - OliverRensu/DeepMIM"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2311.17049"},"[2311.17049] MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://github.com/bytedance/fc-clip"},"GitHub - bytedance/fc-clip: [NeurIPS 2023] This repo contains the code for our paper Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2307.16634"},"[2307.16634] CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2309.05551"},"[2309.05551] OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data")))),(0,n.kt)("h2",{id:"vision-to-language"},"Vision to Language"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2306.07915v3"},"[2306.07915v3] Image Captioners Are Scalable Vision Learners Too")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2311.03079"},"[2311.03079] CogVLM: Visual Expert for Pretrained Language Models"))),(0,n.kt)("h1",{id:"datasets"},"Datasets"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/rom1504/laion-prepro"},"https://github.com/rom1504/laion-prepro")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/allenai/mmc4"},"GitHub - allenai/mmc4: MultimodalC4 is a multimodal extension of c4 that interleaves millions of images with text."))))}m.isMDXComponent=!0}}]);