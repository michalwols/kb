# Vision Language Pre Training

[GitHub - microsoft/BridgeTower: Open source code for AAAI 2023 Paper "BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning"](https://github.com/microsoft/BridgeTower)

[GitHub - microsoft/react](https://github.com/microsoft/react)

https://github.com/salesforce/lavis

https://github.com/uta-smile/TCL

https://github.com/YehLi/xmodaler


https://github.com/sangminwoo/awesome-vision-and-language


[Masked Vision and Language Modeling for Multi-modal Representation Learning (2022-08-03)](https://arxiv.org/abs/2208.02131)


[GitHub - facebookresearch/CiT: Code for the paper titled "CiT Curation in Training for Effective Vision-Language Data".](https://github.com/facebookresearch/CiT/tree/main)

[GitHub - guilk/VLC: Research code for "Training Vision-Language Transformers from Captions Alone"](https://github.com/guilk/VLC)

[GitHub - OliverRensu/TinyMIM](https://github.com/OliverRensu/TinyMIM)

[GitHub - RERV/UniAdapter](https://github.com/RERV/UniAdapter)![[Screen Shot 2023-04-19 at 1.13.36 PM.png]

## Contrastive

### CLIP

- https://github.com/mlfoundations/open_clip
- [GitHub - baaivision/EVA: EVA Series: Vision Foundation Model Fanatics from BAAI](https://github.com/baaivision/EVA)
- https://github.com/OpenAI/CLIP
- https://github.com/LAION-AI/CLIP_benchmark
- https://github.com/rom1504/clip-retrieval
- https://github.com/rom1504/laion-prepro
- [GitHub - yzhuoning/Awesome-CLIP: Awesome list for research on CLIP (Contrastive Language-Image Pre-Training).](https://github.com/yzhuoning/Awesome-CLIP)

- [e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce](http://arxiv.org/abs/2207.00208)


- [GitHub - facebookresearch/SLIP: Code release for SLIP Self-supervision meets Language-Image Pre-training](https://github.com/facebookresearch/SLIP)
- [GitHub - facebookresearch/flip: Official Open Source code for "Scaling Language-Image Pre-training via Masking"](https://github.com/facebookresearch/flip)
- [GitHub - facebookresearch/diht: Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training](https://github.com/facebookresearch/diht)

- [GitHub - OliverRensu/DeepMIM](https://github.com/OliverRensu/DeepMIM)
- [[2311.17049] MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training](https://arxiv.org/abs/2311.17049)

- [GitHub - bytedance/fc-clip: [NeurIPS 2023] This repo contains the code for our paper Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP](https://github.com/bytedance/fc-clip)
- [[2307.16634] CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification](https://arxiv.org/abs/2307.16634)
- [[2309.05551] OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data](https://arxiv.org/abs/2309.05551)


## Vision to Language

- [[2306.07915v3] Image Captioners Are Scalable Vision Learners Too](https://arxiv.org/abs/2306.07915v3)
- [[2311.03079] CogVLM: Visual Expert for Pretrained Language Models](https://arxiv.org/abs/2311.03079)

# Datasets

- https://github.com/rom1504/laion-prepro
- [GitHub - allenai/mmc4: MultimodalC4 is a multimodal extension of c4 that interleaves millions of images with text.](https://github.com/allenai/mmc4)