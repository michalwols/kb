
# Fine Tuning / Transfer Learning



## Parameter efficient transfer learning

- [[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)
- [GitHub - calpt/awesome-adapter-resources: Collection of Tools and Papers related to Adapters (aka Parameter-Efficient Transfer Learning/ Fine-Tuning)](https://github.com/calpt/awesome-adapter-resources)
- [GitHub - microsoft/LoRA: Code for loralib, an implementation of "LoRA: Low-Rank Adaptation of Large Language Models"](https://github.com/microsoft/LoRA)
- [GitHub - huggingface/peft: ðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft)


- [Parameter-Efficient Fine-Tuning using ðŸ¤— PEFT](https://huggingface.co/blog/peft)

- [[2310.08659] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)
- [[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [[2307.05695] Stack More Layers Differently: High-Rank Training Through Low-Rank Updates](https://arxiv.org/abs/2307.05695)

#### Papers

- [[2304.04947] Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference](https://arxiv.org/abs/2304.04947)