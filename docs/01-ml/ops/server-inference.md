# Server Inference


## Triton-inference-server

- [Serving a Torch-TensorRT model with Triton â€” Torch-TensorRT v1.4.0.dev0+d0af394 documentation](https://pytorch.org/TensorRT/tutorials/serving_torch_tensorrt_with_triton.html#serving-torch-tensorrt-with-triton)


## ONNX Runtime

https://github.com/microsoft/onnxruntime-inference-examples



https://github.com/microsoft/DeepSpeed-MII



https://github.com/microsoft/onnx-script




[GitHub - open-mmlab/mmdeploy: OpenMMLab Model Deployment Framework](https://github.com/open-mmlab/mmdeploy)